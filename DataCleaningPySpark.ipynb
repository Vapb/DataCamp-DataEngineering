{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Schema for importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading parquet files\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "df = spark.read.parquet('filename.parquet')\n",
    "\n",
    "# Writing parquet files\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.write.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional DataFrame column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names)\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding ID to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex processing and data pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a9c283881288bc8345488d89afcea2916cc52ed9130ad0e473a7427fa4f195f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
