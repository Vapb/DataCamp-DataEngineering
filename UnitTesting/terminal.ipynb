{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnitTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: c:\\Users\\vapb7\\Documents\\DataCamp-DataEngineering\\UnitTesting\n",
      "plugins: anyio-2.2.0\n",
      "collected 1 item\n",
      "\n",
      "test_convert_to_int.py F                                                 [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "________________________ test_on_string_with_one_comma ________________________\n",
      "\n",
      "    def test_on_string_with_one_comma():\n",
      "      # Complete the assert statement\n",
      ">     assert convert_to_int(\"2,081\") == 2081\n",
      "E     AssertionError: assert '2081' == 2081\n",
      "E      +  where '2081' = convert_to_int('2,081')\n",
      "\n",
      "test_convert_to_int.py:10: AssertionError\n",
      "============================== warnings summary ===============================\n",
      "..\\..\\..\\anaconda3\\lib\\site-packages\\pyreadline\\py3k_compat.py:8\n",
      "  C:\\Users\\vapb7\\anaconda3\\lib\\site-packages\\pyreadline\\py3k_compat.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "    return isinstance(x, collections.Callable)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_convert_to_int.py::test_on_string_with_one_comma - AssertionError...\n",
      "======================== 1 failed, 1 warning in 0.20s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix Convert_to_Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: c:\\Users\\vapb7\\Documents\\DataCamp-DataEngineering\\UnitTesting\n",
      "plugins: anyio-2.2.0\n",
      "collected 1 item\n",
      "\n",
      "test_convert_to_int.py .                                                 [100%]\n",
      "\n",
      "============================== warnings summary ===============================\n",
      "..\\..\\..\\anaconda3\\lib\\site-packages\\pyreadline\\py3k_compat.py:8\n",
      "  C:\\Users\\vapb7\\anaconda3\\lib\\site-packages\\pyreadline\\py3k_compat.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "    return isinstance(x, collections.Callable)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "======================== 1 passed, 1 warning in 0.04s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read tests like documention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "!cat test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoreUnitTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Custom Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: c:\\Users\\vapb7\\Documents\\DataCamp-DataEngineering\\UnitTesting\n",
      "plugins: anyio-2.2.0\n",
      "collected 1 item\n",
      "\n",
      "test_convert_to_int.py F                                                 [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "________________________ test_on_string_with_one_comma ________________________\n",
      "\n",
      "    def test_on_string_with_one_comma():\n",
      "      test_argument = \"2,081\"\n",
      "      expected = 2081\n",
      "      actual = convert_to_int(test_argument)\n",
      "      message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
      ">     assert actual == expected, message\n",
      "E     AssertionError: convert_to_int('2,081') should return the int 2081, but it actually returned 2081\n",
      "E     assert '2081' == 2081\n",
      "\n",
      "test_convert_to_int.py:13: AssertionError\n",
      "============================== warnings summary ===============================\n",
      "..\\..\\..\\anaconda3\\lib\\site-packages\\pyreadline\\py3k_compat.py:8\n",
      "  C:\\Users\\vapb7\\anaconda3\\lib\\site-packages\\pyreadline\\py3k_compat.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "    return isinstance(x, collections.Callable)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_convert_to_int.py::test_on_string_with_one_comma - AssertionError...\n",
      "======================== 1 failed, 1 warning in 0.20s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To comparer Floats use Pytest.Approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "print( 0.1 + 0.1 + 0.1 == 0.3)\n",
    "print( pytest.approx(0.1 + 0.1 + 0.1) == 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using multiple Asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Errors In tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "try:\n",
    "    # Fill in with a context manager that raises Failed if no OSError is raised\n",
    "    with pytest.raises(OSError):\n",
    "        raise ValueError\n",
    "except:\n",
    "    print(\"pytest raised an exception because no OSError was raised in the context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "# Check if the raised ValueError contains the correct message\n",
    "assert exc_info.match(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tests for these cases\n",
    "<li> \n",
    "<ul>Bad Arguments (Arguments that Return error instead of value)</ul>\n",
    "<ul>Special Arguments (Boundary value / Special Logic)</ul>\n",
    "<ul>Normal Arguments</ul>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TDD write unit tests before the code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Organization and Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Test Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)\n",
    "    \n",
    "    # Add more test from split_into_training_and_testing_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd tests <br>\n",
    "pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytests will run all codes with \"test_\" files and will print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest -x  (Will stop when find error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest data/test_preprocessers.py   (will run only test inside this python file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest data/test_preprocessers.py::\"test class name\"  (Run only one class from file)<br>\n",
    "pytest data/test_preprocessers.py::\"test class name\"::\"unit test name\" (Run specific test in file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest -k patern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you know a test will fail and you want to skip it (Expected to fail) (Xfail decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "class TestTrainModel(object):\n",
    "    @pytest.mark.xfail(reason='(TDD) Function yet not implemented')\n",
    "    def test_on_linear_data(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to skip test based on a condition (skipif decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import sys\n",
    "\n",
    "class TestTrainModel(object):\n",
    "    @pytest.mark.skipif(boolean_expression) # if boolean_exp True test is skip\n",
    "    def test_on_linear_data(self):\n",
    "        pass\n",
    "\n",
    "# Real example\n",
    "class TestTrainModel(object):\n",
    "    @pytest.mark.skipif(sys.version_info > (2,7), reason='requires Python 2.7')\n",
    "    def test_on_linear_data(self):\n",
    "        pass # Code that only runs in python 2.7 or Lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest -rs  (Show Reason for skiping)  <br>\n",
    "pytest -rx  (Show Reason for Xfail) <br>\n",
    "pytest -rsx (Show Both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skiping Entire Classes\n",
    "\n",
    "@pytest.mark.skipif\n",
    "class TestTrainModel(object):\n",
    "    pass\n",
    "\n",
    "@pytest.mark.xfail\n",
    "class TestTrainModel(object):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models, Plots and Much More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Work Flow : Setup -> Assert -> Teardown <br>\n",
    "Example of fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixture Structure\n",
    "import pytest \n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    # Do setup here\n",
    "    yield data \n",
    "    # Do Teardown\n",
    "\n",
    "def test_something(my_fixture):\n",
    "    ...\n",
    "    data = my_fixture\n",
    "    ...\n",
    "\n",
    "# Example (Test that need to create a file to test function)\n",
    "import os\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data_file():\n",
    "    # Building files\n",
    "    raw_data_path = 'raw.txt'\n",
    "    clean_data_path = 'clean.txt'\n",
    "    with open(raw_data_path, 'w') as f:\n",
    "        f.write('1,801\\t201,411\\n'\n",
    "                '1,76754\\t12222\\n'\n",
    "                '20,200\\t,20456\\n')\n",
    "    yield raw_data_path, clean_data_path\n",
    "    # Teardown files\n",
    "    os.remove(raw_data_path)\n",
    "    os.remove(clean_data_path)\n",
    "\n",
    "\n",
    "def test_on_raw_data(raw_and_clean_data_file):\n",
    "    raw_path, clean_path = raw_and_clean_data_file()\n",
    "    preprocess(raw_path, clean_path)\n",
    "    with open(clean_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    firstline = lines[0]\n",
    "    assert firstline == '1801\\t201411\\n'\n",
    "    secondline = lines[1]\n",
    "    assert secondline == '176754\\t12222\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmpdir example <br>\n",
    "setup tmpdir() -> setup raw_and_clean_data_file() -> test -> teardown of raw_and_clean_data_file() -> teardown tmpdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def raw_and_clean_data_file(tmpdir):\n",
    "    # Building files\n",
    "    raw_data_path = tmpdir.join('raw.txt')\n",
    "    clean_data_path = tmpdir.join('clean.txt')\n",
    "    with open(raw_data_path, 'w') as f:\n",
    "        f.write('1,801\\t201,411\\n'\n",
    "                '1,76754\\t12222\\n'\n",
    "                '20,200\\t,20456\\n')\n",
    "    yield raw_data_path, clean_data_path\n",
    "    # No teardown nescessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocking : Testing functions independetly of dependencies (pytest-mock , unittest.mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dictionary holding the correct return values \n",
    "    return_values = {\"1,801\": 1801,\n",
    "                     \"201,411\": 201411,\n",
    "                     \"2,002\": 2002,\n",
    "                     \"333,209\": 333209,\n",
    "                     \"1990\": None,\n",
    "                     \"782,911\": 782911,\n",
    "                     \"1,285\": 1285,\n",
    "                     \"389129\": None}\n",
    "    # Return the correct result using the dictionary return_values\n",
    "    return return_values[comma_separated_integer_string]\n",
    "\n",
    "\n",
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from models.train import model_test\n",
    "\n",
    "def test_on_perfect_fit():\n",
    "    # Assign to a NumPy array containing a linear testing set\n",
    "    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
    "    expected = 1\n",
    "    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot testing with pytest-mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGetPlotForBestFitLine(object):\n",
    "    # Add the pytest marker which generates baselines and compares images\n",
    "    @pytest.mark.mpl_image_compare\n",
    "    def test_plot_for_almost_linear_data(self):\n",
    "        slope = 5.0\n",
    "        intercept = -2.0\n",
    "        x_array = np.array([1.0, 2.0, 3.0])\n",
    "        y_array = np.array([3.0, 8.0, 11.0])\n",
    "        title = \"Test plot for almost linear data\"\n",
    "        # Return the matplotlib figure returned by the function under test\n",
    "        return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a9c283881288bc8345488d89afcea2916cc52ed9130ad0e473a7427fa4f195f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
